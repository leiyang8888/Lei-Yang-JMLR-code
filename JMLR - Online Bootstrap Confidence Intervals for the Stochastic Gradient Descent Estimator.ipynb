{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class CI:\n",
    "    def __init__(self, beta_point_est,beta_variance):\n",
    "        #Remember the beta estimation, dimension and duplication times\n",
    "        p=beta_point_est.shape[0]\n",
    "        self.beta_point_est=beta_point_est\n",
    "        self.beta_variance=beta_variance\n",
    "        self.p=p\n",
    " \n",
    "\n",
    "    def quantile(self):\n",
    "        #Calculate the median, 97.5 and 2.5 quantile cut point\n",
    "        beta_point_est=self.beta_point_est\n",
    "        beta_variance=self.beta_variance\n",
    "        p=self.p\n",
    "        str=\" The estimation and 95 percent confidence interval are reported.\\n\"\n",
    "        for i in range(p):\n",
    "            str=str+\" The estimation for dimension %d\"%(i+1)\n",
    "            str=str+\" is %f.\"%beta_point_est[i,0]\n",
    "            beta_i_2_5=beta_point_est[i,0]-1.96*beta_variance[i,0]**(1/2)\n",
    "            beta_i_97_5=beta_point_est[i,0]+1.96*beta_variance[i,0]**(1/2)\n",
    "            str=str+\" The 95 percent confidence interval is [%f,\"%beta_i_2_5\n",
    "            str=str+\" %f].\\n\"%beta_i_97_5\n",
    "        return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "##Our method##\n",
    "##############\n",
    "#####################################\n",
    "##Functions for estimate CI for GLM##\n",
    "#####################################\n",
    "def generate_glm(n,p,beta):\n",
    "    #The function to generate GLM data\n",
    "    #The input beta is matrix\n",
    "    #The input n,p are scalar\n",
    "    X=np.random.normal(0,1,[n,p])\n",
    "    X=np.mat(X)\n",
    "    Y=np.zeros([n,1])\n",
    "    for i in range(n):\n",
    "        eua=np.exp(beta*X[i,:].T)\n",
    "        prob=eua/(1+eua)\n",
    "        Y[i]=np.random.binomial(1,prob,1)\n",
    "    Output=np.zeros([n,p+1])\n",
    "    Output=np.mat(Output)\n",
    "    Output[:,-1]=Y\n",
    "    Output[:,range(p)]=X\n",
    "    #In output, the first p columns are X and last column is Y\n",
    "    return Output\n",
    "\n",
    "\n",
    "def Update_beta_glm(beta_current,X,Y,w,d,iteration,eta,alpha):\n",
    "    #The function for updating beta_current column by column\n",
    "    for i in range(d):\n",
    "        #Calculate the first derivative\n",
    "        eua=np.exp(X*beta_current[:,i])\n",
    "        first_d=X.T*(eua/(1+eua)-Y)\n",
    "        beta_current[:,i]=beta_current[:,i]-w[i]*eta*iteration**(-alpha)*first_d\n",
    "    return beta_current\n",
    "\n",
    "\n",
    "def beta_glm(data,p,m,eta,alpha,beta_base):\n",
    "    #The function to calculate beta estimation variance\n",
    "    #beta_current is used to save current estimation and beta_sum is used to save beta summation. d is the bootstrap number.\n",
    "    d=200\n",
    "    beta_current=np.zeros([p,d])\n",
    "    beta_current=np.mat(beta_current)\n",
    "    beta_sum=np.zeros([p,d])\n",
    "    beta_sum=np.mat(beta_sum)\n",
    "\n",
    "\n",
    "    #Initial bata_current and beta_sum by zeros\n",
    "    for time in range(d):\n",
    "        beta_current[:,time]=beta_base\n",
    "        beta_sum[:,time]=np.mat(np.zeros([1,p])).T\n",
    "\n",
    "    iteration=1\n",
    "    while(iteration<=m):\n",
    "        X=data[(iteration-1),range(p)]\n",
    "        Y=data[(iteration-1),-1]\n",
    "        #w is the weight for bootstrap\n",
    "        w=np.random.exponential(1,d)\n",
    "        #Update unknown parameter estimation\n",
    "        beta_current=Update_beta_glm(beta_current,X,Y,w,d,iteration,eta,alpha)\n",
    "        #Update beta_sum\n",
    "        beta_sum=beta_sum+beta_current\n",
    "        iteration=iteration+1\n",
    "    return beta_sum/m\n",
    "\n",
    "\n",
    "#################################################################################################################################################\n",
    "####################\n",
    "##Point estimation##\n",
    "####################\n",
    "def Update_beta_glm_point(beta_current,X,Y,iteration,eta,alpha):\n",
    "    #The function for updating beta_current\n",
    "    #Calculate the first derivative\n",
    "    eua=np.exp(X*beta_current)\n",
    "    first_d=X.T*(eua/(1+eua)-Y)\n",
    "    beta_current=beta_current-eta*iteration**(-alpha)*first_d\n",
    "    return beta_current\n",
    "\n",
    "\n",
    "def beta_glm_point(data,p,m,eta,alpha,beta_base):\n",
    "    #The function to calculate final beta estimation\n",
    "    #beta_current is used to save current estimation and beta_all is used to save beta estimation process.\n",
    "    beta_current=np.zeros([p,1])\n",
    "    beta_current=np.mat(beta_current)\n",
    "    beta_sum=np.zeros([p,1])\n",
    "    beta_sum=np.mat(beta_sum)\n",
    "\n",
    "\n",
    "    #Initial bata_current and beta_sum by zeros\n",
    "    beta_current=beta_base\n",
    "    beta_all=np.mat(np.zeros([p,m]))\n",
    "\n",
    "    iteration=1\n",
    "    while(iteration<=m):\n",
    "        X=data[(iteration-1),range(p)]\n",
    "        Y=data[(iteration-1),-1]\n",
    "        #Update unknown parameter estimation\n",
    "        beta_current=Update_beta_glm_point(beta_current,X,Y,iteration,eta,alpha)\n",
    "        #Remember beta estimation\n",
    "        beta_all[:,iteration-1]=beta_current\n",
    "        iteration=iteration+1\n",
    "    return np.mean(beta_all,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "##Our method##\n",
    "##############\n",
    "#####################################\n",
    "##Functions for estimate CI for LAD##\n",
    "#####################################\n",
    "def generate_lad(n,p,beta):\n",
    "    #The function to generate linear data\n",
    "    #The input beta is matrix\n",
    "    #The input n,p,sigma are scalar\n",
    "    X=np.random.normal(0,1,[n,p])\n",
    "    X=np.mat(X)\n",
    "    Error=np.random.laplace(0,1,n)\n",
    "    Error=np.mat(Error)\n",
    "    Y=X*beta.T+Error.T\n",
    "    Output=np.zeros([n,p+1])\n",
    "    Output=np.mat(Output)\n",
    "    Output[:,-1]=Y\n",
    "    Output[:,range(p)]=X\n",
    "    #In output, the first p columns are X and last column is Y\n",
    "    return Output\n",
    "\n",
    "\n",
    "def Update_beta_lad(beta_current,X,Y,w,d,iteration,eta,alpha):\n",
    "    #The function for updating beta_current column by column\n",
    "    for i in range(d):\n",
    "        first_d=X.T*np.sign(Y-X*beta_current[:,i])*(-1)\n",
    "        beta_current[:,i]=beta_current[:,i]-w[i]*eta*iteration**(-alpha)*first_d\n",
    "    return beta_current\n",
    "\n",
    "\n",
    "def beta_lad(data,p,m,eta,alpha,beta_base):\n",
    "    #The function to calculate final beta estimation variance\n",
    "    #beta_current is used to save current estimation and beta_sum is used to save beta summation. d is the bootstrap number.\n",
    "    d=200\n",
    "    beta_current=np.zeros([p,d])\n",
    "    beta_current=np.mat(beta_current)\n",
    "    beta_sum=np.zeros([p,d])\n",
    "    beta_sum=np.mat(beta_sum)\n",
    "\n",
    "\n",
    "    #Initial bata_current and beta_sum by zeros\n",
    "    for time in range(d):\n",
    "        beta_current[:,time]=beta_base\n",
    "        beta_sum[:,time]=np.mat(np.zeros([1,p])).T\n",
    "\n",
    "    iteration=1\n",
    "    while(iteration<=m):\n",
    "        X=data[(iteration-1),range(p)]\n",
    "        Y=data[(iteration-1),-1]\n",
    "        #w is the weight for bootstrap\n",
    "        w=np.random.exponential(1,d)\n",
    "        #Update unknown parameter estimation\n",
    "        beta_current=Update_beta_lad(beta_current,X,Y,w,d,iteration,eta,alpha)\n",
    "        #Update beta_sum\n",
    "        beta_sum=beta_sum+beta_current\n",
    "        iteration=iteration+1\n",
    "    return beta_sum/m \n",
    "\n",
    "\n",
    "\n",
    "#################################################################################################################################################\n",
    "####################\n",
    "##Point estimation##\n",
    "####################\n",
    "def Update_beta_lad_point(beta_current,X,Y,iteration,eta,alpha):\n",
    "    #The function for updating beta_current \n",
    "    first_d=X.T*np.sign(Y-X*beta_current)*(-1)\n",
    "    beta_current=beta_current-eta*iteration**(-alpha)*first_d\n",
    "    return beta_current\n",
    "\n",
    "\n",
    "def beta_lad_point(data,p,m,eta,alpha,beta_base):\n",
    "    #The function to calculate final beta estimation\n",
    "    #beta_current is used to save current estimation and beta_all is used to save beta estiamtion process.\n",
    "    beta_current=np.zeros([p,1])\n",
    "    beta_current=np.mat(beta_current)\n",
    "    beta_sum=np.zeros([p,1])\n",
    "    beta_sum=np.mat(beta_sum)\n",
    "\n",
    "\n",
    "    #Initial bata_current and beta_sum by zeros\n",
    "    beta_current=beta_base\n",
    "    beta_all=np.mat(np.zeros([p,m]))\n",
    "\n",
    "    iteration=1\n",
    "    while(iteration<=m):\n",
    "        X=data[(iteration-1),range(p)]\n",
    "        Y=data[(iteration-1),-1]\n",
    "        #Update unknown parameter estimation\n",
    "        beta_current=Update_beta_lad_point(beta_current,X,Y,iteration,eta,alpha)\n",
    "        #Remember beta estimation\n",
    "        beta_all[:,iteration-1]=beta_current\n",
    "        iteration=iteration+1\n",
    "    return np.mean(beta_all,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "##Functions for estimate CI for linear model##\n",
    "##############################################\n",
    "def generate_linear(n,p,beta,sigma):\n",
    "    #The function to generate linear data\n",
    "    #The input beta is matrix\n",
    "    #The input n,p,sigma are scalar\n",
    "    X=np.random.normal(0,1,[n,p])\n",
    "    X=np.mat(X)\n",
    "    Error=np.random.normal(0,sigma,n)\n",
    "    Error=np.mat(Error)\n",
    "    Y=X*beta.T+Error.T\n",
    "    Output=np.zeros([n,p+1])\n",
    "    Output=np.mat(Output)\n",
    "    Output[:,-1]=Y\n",
    "    Output[:,range(p)]=X\n",
    "    #In output, the first p columns are X and last column is Y\n",
    "    return Output\n",
    "\n",
    "\n",
    "def Update_beta_linear(beta_current,X,Y,w,d,iteration,eta,alpha):\n",
    "    #The function for updating beta_current column by column\n",
    "    for i in range(d):\n",
    "        first_d=X.T*(Y-X*beta_current[:,i])*(-1)\n",
    "        beta_current[:,i]=beta_current[:,i]-w[i]*eta*iteration**(-alpha)*first_d\n",
    "    return beta_current\n",
    "\n",
    "\n",
    "def beta_linear(data,p,m,eta,alpha,beta_base):\n",
    "    #The function to calculate estimation variance\n",
    "    #beta_current is used to save current estimation and beta_sum is used to save beta summation. d is the bootstrap number.\n",
    "    d=200\n",
    "    beta_current=np.zeros([p,d])\n",
    "    beta_current=np.mat(beta_current)\n",
    "    beta_sum=np.zeros([p,d])\n",
    "    beta_sum=np.mat(beta_sum)\n",
    "\n",
    "    #Initial bata_current and beta_sum by zeros\n",
    "    for time in range(d):\n",
    "        beta_current[:,time]=beta_base\n",
    "        beta_sum[:,time]=np.mat(np.zeros([1,p])).T\n",
    "\n",
    "    iteration=1\n",
    "    while(iteration<=m):\n",
    "        X=data[(iteration-1),range(p)]\n",
    "        Y=data[(iteration-1),-1]\n",
    "        #w is the weight for bootstrap\n",
    "        w=np.random.exponential(1,d)\n",
    "        #Update unknown parameter estimation\n",
    "        beta_current=Update_beta_linear(beta_current,X,Y,w,d,iteration,eta,alpha)\n",
    "        #Update beta_sum\n",
    "        beta_sum=beta_sum+beta_current\n",
    "        iteration=iteration+1\n",
    "    return beta_sum/m  \n",
    "\n",
    "\n",
    "######################################\n",
    "##The function for removing outliers##\n",
    "######################################\n",
    "def remove_outlier(x):\n",
    "    total_count=np.size(x,1)\n",
    "    y=[]\n",
    "    ##Calculate median and IQR\n",
    "    q_50=np.percentile(x,50)\n",
    "    q_25=np.percentile(x,25)\n",
    "    q_75=np.percentile(x,75)\n",
    "    IQR=q_75-q_25\n",
    "    low=q_25-1.5*IQR\n",
    "    high=q_75+1.5*IQR\n",
    "    ##Remove outliers\n",
    "    for i in range(total_count):\n",
    "        if x[0,i]>low and x[0,i]<high:\n",
    "            y.append(x[0,i])\n",
    "    return y\n",
    "    \n",
    "    \n",
    "#####################################\n",
    "##Calculate the estimation variance##\n",
    "##################################### \n",
    "def beta_var(beta_estimation):\n",
    "    p=np.size(beta_estimation,0)\n",
    "    beta_variance=np.mat(np.zeros([p,1]))\n",
    "    #Estimate the estimation variance \n",
    "    for i in range(p):\n",
    "        beta_variance[i]=np.var(remove_outlier(beta_estimation[i,:]))\n",
    "    return beta_variance\n",
    "\n",
    "\n",
    "#################################################################################################################################################\n",
    "####################\n",
    "##Point estimation##\n",
    "####################\n",
    "def Update_beta_linear_point(beta_current,X,Y,iteration,eta,alpha):\n",
    "    #The function for updating beta_current \n",
    "    first_d=X.T*(Y-X*beta_current)*(-1)\n",
    "    beta_current=beta_current-eta*iteration**(-alpha)*first_d\n",
    "    return beta_current\n",
    "\n",
    "\n",
    "def beta_linear_point(data,p,m,eta,alpha,beta_base):\n",
    "    #The function to calculate final beta estimation\n",
    "    #beta_current is used to save current estimation and beta_all is used to save beta estimation process.\n",
    "    beta_current=np.zeros([p,1])\n",
    "    beta_current=np.mat(beta_current)\n",
    "    beta_sum=np.zeros([p,1])\n",
    "    beta_sum=np.mat(beta_sum)\n",
    "\n",
    "    #Initial bata_current and beta_sum by nzeros\n",
    "    beta_current=beta_base\n",
    "    beta_all=np.mat(np.zeros([p,m]))\n",
    "\n",
    "    iteration=1\n",
    "    while(iteration<=m):\n",
    "        X=data[(iteration-1),range(p)]\n",
    "        Y=data[(iteration-1),-1]\n",
    "        #Update unknown parameter estimation\n",
    "        beta_current=Update_beta_linear_point(beta_current,X,Y,iteration,eta,alpha)\n",
    "        #Remember beta estimation\n",
    "        beta_all[:,iteration-1]=beta_current\n",
    "        iteration=iteration+1\n",
    "    return np.mean(beta_all,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class beta_hist:\n",
    "    def __init__(self, beta_estimation):\n",
    "        #Remember the beta estimation, dimension and duplication times\n",
    "        p=beta_estimation.shape[0]\n",
    "        d=beta_estimation.shape[1]\n",
    "        self.beta_estimation=beta_estimation\n",
    "        self.p=p\n",
    "        self.d=d\n",
    "\n",
    "\n",
    "    def beta_hist_plot(self):\n",
    "        #Draw the histogram for each dimension\n",
    "        p=self.p\n",
    "        d=self.d\n",
    "        beta_estimation=self.beta_estimation\n",
    "        hist_list=np.zeros(d)\n",
    "        for i in range(p):\n",
    "            #Change the format from matrix to array\n",
    "            for j in range(d):\n",
    "                hist_list[j]=beta_estimation[i,j]\n",
    "            plt.subplot(int(p**(1/2))+1,int(p**(1/2))+1,i+1)\n",
    "            beta_pic=plt.hist(hist_list,20,range=[np.percentile(hist_list,50)-0.1,np.percentile(hist_list,50)+0.1])\n",
    "            plt.xlabel(\"Value\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.title(\"Histogram for dimension %d\"%(i+1))\n",
    "        return beta_pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What kind of model would you like? 0 indicates linear model, 1 indicates GLM and 2 indicates LAD.\n",
      "0\n",
      "What is dimension of your unknown parameters?\n",
      "2\n",
      "What is coefficient of element 1 in unknown parameter?\n",
      "1\n",
      "What is coefficient of element 2 in unknown parameter?\n",
      "-1\n",
      "What is the standard deviation of basement data error?\n",
      "0.5\n",
      "What is total wave number of following data?\n",
      "2000\n",
      "What is the tuning parameter eta in stochastic gradient learning?\n",
      "1\n",
      "What is the tuning parameter alpha in stochastic gradient learning?\n",
      "0.66\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The estimation and 95 percent confidence interval are reported.\n",
      " The estimation for dimension 1 is 1.021415. The 95 percent confidence interval is [0.964942, 1.077887].\n",
      " The estimation for dimension 2 is -1.013868. The 95 percent confidence interval is [-1.087525, -0.940211].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def justify_input(integer):\n",
    "    #Justify if the input is integer\n",
    "    flag=0\n",
    "    if integer.isdigit()==False:\n",
    "        flag=1\n",
    "    else:\n",
    "        flag=0\n",
    "    #If the input is integer, flag=0. Otherwise flag=1\n",
    "    return flag\n",
    "    \n",
    "\n",
    "def justify_model(zero_one_two):\n",
    "    #Justify if the input is zero or one\n",
    "    flag=0\n",
    "    if zero_one_two.isdigit()==False:\n",
    "        flag=1\n",
    "    elif int(zero_one_two)<0 or int(zero_one_two)>2:\n",
    "        flag=1\n",
    "    else:\n",
    "        flag=0\n",
    "    #If the input is zero, one or two, flag=0. Otherwise flag=1\n",
    "    return flag\n",
    "        \n",
    "def main():\n",
    "    #Denote the linear, GLM or lad model\n",
    "    model_name=input(\"What kind of model would you like? 0 indicates linear model, 1 indicates GLM and 2 indicates LAD.\\n\")\n",
    "    flag=justify_model(model_name)\n",
    "    #If the input is not correct, the program will ask the user to input again till correct\n",
    "    while(flag!=0):\n",
    "        model_name=input(\"The last input is not valid. What kind of model would you like? 0 indicates linear model and 1 indicates GLM.\\n\")\n",
    "        flag=justify_model(model_name)\n",
    "    if(int(model_name)==0):\n",
    "        #The dimension of unknown parameter\n",
    "        p=input(\"What is dimension of your unknown parameters?\\n\")\n",
    "        flag=justify_input(p)\n",
    "        #If the input is not correct, the program will ask the user to input again till correct\n",
    "        while(flag!=0):\n",
    "            p=input(\"The last input is not valid. What is dimension of your unknown parameters?\\n\")\n",
    "            flag=justify_input(p)\n",
    "        p=int(p)\n",
    "\n",
    "        #Let the user input the unknown parameter element by element\n",
    "        beta=list(range(p))\n",
    "        for i in range(p):\n",
    "            beta_i=input(\"What is coefficient of element %d in unknown parameter?\\n\"%(i+1))\n",
    "            beta[i]=float(beta_i)\n",
    "        beta=np.mat(beta)\n",
    "\n",
    "        #The error term in linear model\n",
    "        sigma=input(\"What is the standard deviation of basement data error?\\n\")\n",
    "        sigma=float(sigma)\n",
    "\n",
    "        m=input(\"What is total wave number of following data?\\n\")\n",
    "        flag=justify_input(m)\n",
    "        #If the input is not correct, the program will ask the user to input again till correct\n",
    "        while(flag!=0):\n",
    "            m=input(\"The last input is not valid. What is total wave number of following data?\\n\")\n",
    "            flag=justify_input(m)\n",
    "        m=int(m)\n",
    "    \n",
    "        eta=input(\"What is the tuning parameter eta in stochastic gradient learning?\\n\")\n",
    "        eta=float(eta)  \n",
    "        alpha=input(\"What is the tuning parameter alpha in stochastic gradient learning?\\n\")\n",
    "        alpha=float(alpha)   \n",
    "\n",
    "        #Estimate unknown parameter beta by stochastic gradient learning\n",
    "        #Give the initial value\n",
    "        beta_base=np.mat(np.zeros([p,1]))\n",
    "        #Generate data\n",
    "        data=generate_linear(m,p,beta,sigma)\n",
    "        #Calculate point estimation\n",
    "        beta_point_est=beta_linear_point(data,p,m,eta,alpha,beta_base)\n",
    "        #Do the bootstrap estimation\n",
    "        beta_estimation=beta_linear(data,p,m,eta,alpha,beta_base)\n",
    "        #Estimate the estimation variance\n",
    "        beta_variance=beta_var(beta_estimation)\n",
    "    elif(int(model_name)==1):\n",
    "        #The dimension of unknown parameter\n",
    "        p=input(\"What is dimension of your unknown parameters?\\n\")\n",
    "        flag=justify_input(p)\n",
    "        #If the input is not correct, the program will ask the user to input again till correct\n",
    "        while(flag!=0):\n",
    "            p=input(\"The last input is not valid. What is dimension of your unknown parameters?\\n\")\n",
    "            flag=justify_input(p)\n",
    "        p=int(p)\n",
    "\n",
    "        #Let the user input the unknown parameter element by element\n",
    "        beta=list(range(p))\n",
    "        for i in range(p):\n",
    "            beta_i=input(\"What is coefficient of element %d in unknown parameter?\\n\"%(i+1))\n",
    "            beta[i]=float(beta_i)\n",
    "        beta=np.mat(beta)\n",
    "\n",
    "        m=input(\"What is total wave number of following data?\\n\")\n",
    "        flag=justify_input(m)\n",
    "        #If the input is not correct, the program will ask the user to input again till correct\n",
    "        while(flag!=0):\n",
    "            m=input(\"The last input is not valid. What is total wave number of following data?\\n\")\n",
    "            flag=justify_input(m)\n",
    "        m=int(m)\n",
    "\n",
    "        eta=input(\"What is the tuning parameter eta in stochastic gradient learning?\\n\")\n",
    "        eta=float(eta)\n",
    "        alpha=input(\"What is the tuning parameter alpha in stochastic gradient learning?\\n\")\n",
    "        alpha=float(alpha)   \n",
    "        \n",
    "        #Give the initial value\n",
    "        beta_base=np.mat(np.zeros([p,1]))\n",
    "        #Generate data\n",
    "        data=generate_glm(m,p,beta)\n",
    "        #Calculate the point estimation\n",
    "        beta_point_est=beta_glm_point(data,p,m,eta,alpha,beta_base)\n",
    "        #Do the bootstrap estimation\n",
    "        beta_estimation=beta_glm(data,p,m,eta,alpha,beta_base)\n",
    "        #Estimate the estimation variance\n",
    "        beta_variance=beta_var(beta_estimation)\n",
    "    elif(int(model_name)==2):\n",
    "        #The dimension of unknown parameter\n",
    "        p=input(\"What is dimension of your unknown parameters?\\n\")\n",
    "        flag=justify_input(p)\n",
    "        #If the input is not correct, the program will ask the user to input again till correct\n",
    "        while(flag!=0):\n",
    "            p=input(\"The last input is not valid. What is dimension of your unknown parameters?\\n\")\n",
    "            flag=justify_input(p)\n",
    "        p=int(p)\n",
    "\n",
    "        #Let the user input the unknown parameter element by element\n",
    "        beta=list(range(p))\n",
    "        for i in range(p):\n",
    "            beta_i=input(\"What is coefficient of element %d in unknown parameter?\\n\"%(i+1))\n",
    "            beta[i]=float(beta_i)\n",
    "        beta=np.mat(beta)\n",
    "\n",
    "        m=input(\"What is total wave number of following data?\\n\")\n",
    "        flag=justify_input(m)\n",
    "        #If the input is not correct, the program will ask the user to input again till correct\n",
    "        while(flag!=0):\n",
    "            m=input(\"The last input is not valid. What is total wave number of following data?\\n\")\n",
    "            flag=justify_input(m)\n",
    "        m=int(m)\n",
    "\n",
    "    \n",
    "        eta=input(\"What is the tuning parameter eta in stochastic gradient learning?\\n\")\n",
    "        eta=float(eta)\n",
    "        alpha=input(\"What is the tuning parameter alpha in stochastic gradient learning?\\n\")\n",
    "        alpha=float(alpha)   \n",
    "        \n",
    "        #Give the initial value\n",
    "        beta_base=np.mat(np.zeros([p,1]))\n",
    "        #Generate data\n",
    "        data=generate_lad(m,p,beta)\n",
    "        #Calculate point estimation\n",
    "        beta_point_est=beta_lad_point(data,p,m,eta,alpha,beta_base)\n",
    "        #Do the bootstrap estimation\n",
    "        beta_estimation=beta_lad(data,p,m,eta,alpha,beta_base)\n",
    "        #Estimate the estimation variance\n",
    "        beta_variance=beta_var(beta_estimation)    \n",
    "    \n",
    "    #Plot the histogram for each dimension\n",
    "    pic=beta_hist(beta_estimation)\n",
    "    pic.beta_hist_plot()\n",
    "    plt.show()\n",
    "    \n",
    "    infor=CI(beta_point_est,beta_variance).quantile()\n",
    "    print(infor)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Keyboard Interruption\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
